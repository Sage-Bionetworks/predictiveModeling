\documentclass[10pt]{article}

\usepackage{times}
\usepackage{hyperref}
\usepackage{float}

\textwidth=6.5in
\textheight=8.5in
\oddsidemargin=-.1in
\evensidemargin=-.1in
\headheight=-.3in

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}
\newcommand{\predictiveModeling}{\Rpackage{predictiveModeling}}
\newcommand{\synapseClient}{\Rpackage{synapseClient}}

\title{Building predictive models of drug sensitivity using Synapse}
\author{Elias Chaibub Neto, Nicole Deflaux}
\date{\today}

\SweaveOpts{keep.source=TRUE}

\begin{document}

\maketitle

\section{Sage Bionetworks Synapse project}

The recent exponential growth of biological ``omics'' data has occurred
concurrently with a decline in the number of NMEs approved by the FDA. Sage
Bionetworks believes that a fundamental reason biological research
productivity does not scale with biological data generation is that the
analysis and interpretation of genomic data remains largely an isolated,
individual activity. Sage Bionetworks' mission is to catalyze a cultural
transition from the traditional single lab, single-company, and
single-therapy R\&D paradigm to a model with broad precompetitive
collaboration on the analysis of large scale data in medical sciences. For
this to happen it is critical that: 1) human health data become accessible
and reusable by people other than the original data generators allowing
multiple parallel approaches to data interpretation; 2) analytical
methodologies become fully reproducible and transparent so that results can
be vetted and existing analysis techniques quickly applied to new
application areas, and; 3) models of biological systems and networks be
opened to a variety of users such that theoretical predictions can be
rapidly validated experimentally and improve standards of care for
patients. Sage Bionetworks is actively engaged with academic and
pharmaceutical collaborators in developing technical, policy and strategic
solutions to these issues. Part of Sage's solution is Synapse, a platform
for open, reproducible data-driven science, which will provide support for
Sage's research initiatives and serve as a resource for the broader
scientific community.

Synapse will support the reusability of information facilitated by
ontology-based services and applications directed at scientific researchers
and data curators. Sage Bionetworks is actively pursuing the acquisition,
curation, statistical quality control, and hosting of human and mouse global
coherent datasets for use by Sage Bionetworks researchers, collaborators,
and the broader research community. Global coherent datasets integrate both
clinical phenotype and genomic data along with an intermediate molecular
layer such as gene expression or proteomic data. Typically studies contain
genome-wide genetic variation data and/or expression profiling data. We
expect the release of these sorts of unique, integrative, high value
datasets into the public domain will seed a variety of analytical approaches
to drive new treatments based on better understanding of disease states and
the biological effects of existing drugs.

Reproducible analysis and disease model reuse require a supporting
informatics infrastructure. In the Sage Bionetworks system, users interact
with resources via a number of mechanisms depending upon their interests and
expertise. The Synapse web portal will be an environment for end user
scientists to interact and share data, models, and analysis methods, both in
the context of specific research projects, and broadly across otherwise
disparate projects. Many other specialized scientific tools can be extended
to load data and save results to the Sage Bionetworks platform, or to
perform analysis by calling methods executed on a remote service. The Sage
Bionetworks Platform is aggressively leveraging and optimizing its
architecture to take full advantage of the rapidly maturing cloud computing
technologies which will put on-demand supercomputing power in the hands of
the average researcher.  These more specialized analytical clients would
support use cases in data curation and quality control as well as scientific
analysis.

\section{Set up your Synapse work environment}

Go to \url{https://synapse-alpha.sagebase.org/} to register for a new
account and then log into Synapse.

\begin{figure}[H]
  \centering
  \includegraphics{synapseScreenshots/Register.png}
  \caption{Register for a Synapse account and log in}
\end{figure}

Use the following R code to setup your Synapse work environment.

<<loadlib>>=
library(predictiveModeling)

# Change the values of these variables 
myName <- "Your First Name and Last Name Here"
myWorkingDirectory <- "."

setwd(myWorkingDirectory)
@

Create a Synapse project to hold your analyses results.  Be sure to type in
your Synapse username and password when prompted from R.

<<fakeLogin,eval=FALSE>>=
library(synapseClient)
synapseLogin()

project <- Project(list(
  name=paste("Machine Learning Results - ", myName)
  ))
project <- createEntity(project)

analysis <- Analysis(list(
	name="ElasticNet versus LinearModel",
	description="Several Machine Learning methods run upon CCLE Data with Sanger Drug Response",
	parentId=propertyValue(project, "id")
	))
analysis <- createEntity(analysis)

dataset <- Dataset(list(
  name="Analysis Plots",
  parentId=propertyValue(project, "id")
  ))
dataset <- createEntity(dataset)
@ 

Go back to \url{https://synapse-alpha.sagebase.org/} and find your newly created
project.  Click on ``share'' and share your project with the group
AUTHENTICATED\_USERS.

\begin{figure}[H]
  \centering
  \includegraphics{synapseScreenshots/Sharing.png}
  \caption{Find your project and share it with AUTHENTICATED\_USERS}
\end{figure}

\section{Load data from Synapse}

Navigate to the ``Cell Line Project'' in Synapse
\url{https://synapse-alpha.sagebase.org/#Project:5019}.  Click on the two
datasets listed there and their layers to get the IDs of the data layers for
CCLE copy number, CCLE oncomap, and Sanger drug response IC50s.  Note
that you can also browse data available in Synapse via the Synapse R Client.
See the help documentation for synapseClient for more detail.

\begin{figure}[H]
  \centering
  \includegraphics{synapseScreenshots/Datasets.png}
  \caption{Find the CCLE and Sanger Dataset Layers in Synapse}
\end{figure}
  
<<fakeLoadData,eval=FALSE>>=

#### Load Data from Synapse ####
id_copyLayer <- "6067"
copyLayer <- loadEntity(id_copyLayer)
ds_copy_ccle <- copyLayer$objects$ds_copy_ccle

id_oncomapLayer <- "6069"
oncomapLayer <- loadEntity(id_oncomapLayer)
ds_oncomap_ccle <- oncomapLayer$objects$ds_oncomap_ccle

id_sangerLayer <- "6084"
sangerLayer <- loadEntity(id_sangerLayer)
ds_chem_ic50s_sanger <- sangerLayer$objects$ds_chem_ic50s_sanger
@ 

<<realLoadData,echo=FALSE>>=
data(demoData)
@ 

<<exploreSangerData>>=
ds_chem_ic50s_sanger <- log10(ds_chem_ic50s_sanger)
print(rownames(ds_chem_ic50s_sanger))
@

Combine the feature data into a single dataset. The number of features in the aggregate dataset should equal the number of features in the copy number data plus the number of features in the oncomap data.
<<>>==
ds_features_cn_mut_ccle <- createAggregateFeatureDataSet(list(copy = ds_copy_ccle, 
                                                              mut = ds_oncomap_ccle))
checkEquals(nrow(ds_features_cn_mut_ccle), nrow(ds_copy_ccle) + nrow(ds_oncomap_ccle))
@

We want to use the aggregate features to build predictive models of drug response. Run the function createFeatureAndResponseDataList, which finds the samples that are common between the feature data and response data and creates subsets of the feature and response datasets containing only these samples, in the same order. These new datasets are returned in a list with elements featureData and responseData. Check that the columns of the datasets returned in the list are the same.
<<>>==
dataSets_ccleFeaturesCnMut_sangerChems <- createFeatureAndResponseDataList(ds_features_cn_mut_ccle, 
                                                                           ds_chem_ic50s_sanger)
ls(dataSets_ccleFeaturesCnMut_sangerChems)
checkEquals(colnames(dataSets_ccleFeaturesCnMut_sangerChems$featureData), 
            colnames(dataSets_ccleFeaturesCnMut_sangerChems$responseData))
@

Normalize the data. The scale function only scales the columns of a matrix, so we transpose the matrices as input to the scale function and then transpose the results back to the original form. We want to build a predictive model on a single compound, so return to responseData\_scaled only the row of responseData corresponding to the compound PLX4720.


<<>>==
featureData_scaled <- t(scale(t(dataSets_ccleFeaturesCnMut_sangerChems$featureData)))
responseData_scaled <- 
  t(scale(t(dataSets_ccleFeaturesCnMut_sangerChems$responseData["PLX4720",,drop=FALSE])))

print(dim(featureData_scaled))
print(dim(responseData_scaled))
@

PLX4720 is an inhibitor of mutant BRAF and is known to be selective for BRAF mutant tumors. Plot the IC50 values for cell lines with wild type and mutant BRAF to confirm that cell lines with mutant BRAF have lower IC50 values on average.

<<fig=TRUE>>=
boxplot(as.numeric(responseData_scaled) ~ featureData_scaled["BRAF_mut",], 
        names = c("BRAF WT", "BRAF Mut"), 
        ylab="PLX4720 IC50", col="blue")
@

\section{Fit elastic net model}
Given that BRAF mutant cell lines have lower IC50 values for PLX4720, can we develop a predictive model that can discover BRAF mutations as predictive of PLX4720 in an unbiased analysis of all of the feature data? To do this, we use the fitPredictiveModel() function, which takes as input feature data, response data, and a modeling algorithm used to build a predictor of response based on the feature data.

By default, we can use any modeling algorithm supported by the caret package. Note that these algorithms use the convention of rows corresponding to samples and columns corresponding to features, so we transpose the data as input. 

In this vignette, we will apply a regularized regression algorithm, called elastic net regression, that is designed to work for problems with many more variables than observations (referred to as $p \gg n$), which is the case in our example where we have tens of thousands of features and only hundreds of samples. Users are encouraged to experiment on their own with other modeling algorithms, as well as customized novel algorithms, as described later in this vignette.

Elastic net overcomes the $p \gg n$ problem by imposing a complexity penalty on the parameters and selecting the strength of this penalty using a cross validation procedure and selecting the parameter leading to the lowest test error in the training data. 

Let's construct an elastic net model, specified using the "glmnet" parameter in caret. For elastic net, and all other models that require parameter tuning, the test error will be computed across many different combinations of training parameters. This grid of tuning parameters is specified in the tuneGrid parameter to fitPredictiveModel. If it is not specified it will be created by default. However, we have included a customized tuneGrid for elastic net that sets one of the parameters (alpha) to 1 (corresponding to lasso regression) and varies the other parameter (lambda) across a range of values. The lambda parameter controls the strength of the penalty imposed on the complexity of the model -- in this case, the L1 norm, or sum of absolute values of the coefficients.

<<>>==
model_eNet <- fitPredictiveModel(t(featureData_scaled), 
                                 t(responseData_scaled), 
                                 method="glmnet", 
                                 tuneGrid=createENetTuneGrid(alphas=1))
@

fitPredictiveModel wraps the caret train function and returns an object of class train. The default plotting method for train objects displays the test error associated with the model parameters used in tuneGrid. To ensure that we have tested the proper range of tuning parameters we would expect the lowest test error to occur at intermediate tuning parameter values, with larger test errors at the low and high tuning parameter values, which cause the model to be overfit and underfit.
<<fig=TRUE>>=
print(plot(model_eNet))
@

The model selected as optimal is returned in the finalModel element of the train object. This model is fit using the tuning parameters that were computed as optimal, and these optimal tuning parameters are specified in the tuneValue element of finalModel. The optimal alpha is 1 (since this is the only alpha value we tested). Ensure that the optimal lambda value corresponds to the lambda producing the minimal test error, as seen in the plot above.
<<>>=
print(model_eNet$finalModel$tuneValue)
@

Elastic net computes coefficients associated with a subset of the features, such that response is modeled
as a weighted sum of these feature values. We can visualize the results of the model by plotting a heatmap of the inferred predictive features and the inferred weights associated with each feature. To do this, extract the coefficients of the model and call the plotPredictiveModelHeatmap function. Note that the elastic net model returns the entire solution path in the beta element, where the final beta values are at the termination of the solution path, and therefore the last column of the beta element.

The colored line at the bottom of the plot displays the sensitivity of each cell line to PLX4720 (in normalized IC50 values), sorted from the most to least sensitive cell line. The heatmap displays each inferred predictive feature, with columns representing cell lines in the same order as the sensitivity plot on the bottom, with red corresponding to high feature values (e.g. mutations or amplifications) and blue corresponding to low feature values. The bar plot to the left of the heatmap displays the weights associated with each feature.

Verify that BRAF mutation is inferred as the most predictive feature. 
<<>>=
coefs_eNet <- model_eNet$finalModel$beta[, ncol(model_eNet$finalModel$beta)]
outputFileElasticNet <- 'PLX4720_ElasticNetModel.jpg'
#plotPredictiveModelHeatmap(coefs_eNet, 
#                           featureData_scaled, 
#                           responseData_scaled)
@
\includegraphics{PLX4720_ElasticNetModel.jpg}

Store the Elastic net analysis result in Synapse.
<<storeElasticNet,eval=FALSE>>=
elasticNetLayer <- Layer(list(
                            name="ElasticNet Results for PLX4720",
                            type="M", 
                            parentId=propertyValue(dataset, "id")))
elasticNetLayer <- addFile(elasticNetLayer, outputFileElasticNet)
storeEntity(elasticNetLayer)

step1 <- stopStep()
onWeb(step1)

propertyValue(step1, 'name') <- "Single run using elastic net"
propertyValue(step1, 'description') <- "I found that ... looked very interesting due to ..."
step1 <- updateEntity(step1)

step2 <- startStep(analysis)
propertyValue(step2, 'name') <- "Cross validation using elastic net"
propertyValue(step2, 'input') <- propertyValue(step1, "input")
step2 <- updateEntity(step2)

onWeb(analysis)


@

\section{Evaluate model performance}
We can evaluate the performance of a predictive model using a cross validation procedure in which parts of the data are successively held out, the model is fit using the remaining data, and the predictions made from the held out features are compared against the held out response data. This cross validation procedure is implemented in the crossValidatePredictiveModel() function. Call this function with the same arguments as fitPredictiveModel() to evaluate the performance of the elastic net regression model.


<<>>=
eNet_cv <- crossValidatePredictiveModel(t(featureData_scaled), 
                                        t(responseData_scaled), 
                                        method="glmnet", 
                                        tuneGrid=createENetTuneGrid(alphas=1), 
                                        numFolds = 10)
@

The value returned from this function, contained in eNet\_cv, is a list containing elements predAndObs\_train and predAndObs\_test. Each element is a data.frame with variables pred and obs, corresponding to the predicted and observed values for the train and test data, respectively.

Compare the predicted and observed values in the train and test data using the functions plotPrediction\_train() and plotPrediction\_test().

<<fig=TRUE>>=
plotPrediction_train(eNet_cv)
@
<<fig=TRUE>>=
plotPrediction_test(eNet_cv)
@

The predictions generalize pretty well from the training to the test data. This is due to the complexity penalty, which controls for overfitting. What would have happened if we constructed a model with a lower complexity penalty and allowed the model to overfit the data? Try this by passing low lambda values as parameters to the createENetTuneGrid function.

<<>>=
eNet_lowLambda_cv <- crossValidatePredictiveModel(t(featureData_scaled), 
                                                  t(responseData_scaled), 
                                                  method="glmnet", 
                                                  tuneGrid=createENetTuneGrid(alphas=1, 
                                                                              lambdas=c(2e-10, 1e-10)), 
                                                  numFolds = 10)
@

<<fig=TRUE>>=
plotPrediction_train(eNet_lowLambda_cv)
@
<<fig=TRUE>>=
plotPrediction_test(eNet_lowLambda_cv)
@

With out adequately controlling for overfitting, the model fits the training data very well, but does not generalize well to the test data and is therefore unlikely to give accurate predictions for new samples.

<<fig=TRUE>>=
boxplot(computeTestErrors(eNet_cv), computeTestErrors(eNet_lowLambda_cv))
t.test(computeTestErrors(eNet_cv), computeTestErrors(eNet_lowLambda_cv))
@

The test error using the optimal complexity penalty is statistically significantly lower than the test error using a low complexity penalty. This provides an objective test to evaluate the relative performance of different models. Note that this single example contains relatively few sensitive cell lines and robust assessments of relative performance will require comparisons across many different predictions (e.g. sensitivity to different drugs).


\section{Advanced topic: Build a custom model}
An active area of research in computational biology applications in cancer medicine is to develop predictive models able to discover relationships between cancer genotypes and drug response, especially for drugs where the association is weaker than between BRAF and PLX4720.

The predictiveModeling package provides a customizable interface for developing novel predictive models and testing their performance against other predictive models. In order to define a custom predictiveModel, we need to create a subclass of the abstract PredictiveModeling class and define two functions -- customTrain() and customPredict(). The customTrain() function uses a training set of feature and response data to fit the parameters of the custom model. The customPredict() function uses the model that was built in customTrain() to predict the expected response for new observations that were not used in the model fitting procedure.

Let's start by implementing a customized version of linear regression as shown in the code below.

<<>>=
setClass("LinearRegression", 
    representation=representation(linearModel="lm"),
  	contains="PredictiveModel",
    prototype = prototype(
			linearModel = {
				nulllm <- list()
				class(nulllm) <- "lm"
				nulllm
			}
	)
)

setMethod(
		f = "customTrain",
		signature = signature("LinearRegression", "matrix", "numeric"),
		definition = function(method, featureData, responseData){
			message("in train_LinearRegression")
			linearModel <- lm(responseData ~ featureData)
			names(linearModel$coefficients)[2:length(linearModel$coefficients)] <- colnames(featureData)
			method@linearModel <- linearModel
			method
		}
)

setMethod(
		f = "customPredict",
		signature = signature("LinearRegression", "matrix"),
		definition = function(method, featureData){
			print("in predict_linearRegresion")
			testCoefs <- method@linearModel$coefficients
			testCoefs <- testCoefs[-1]
			testCoefs <- testCoefs[!is.na(testCoefs)]
			testCoefs <- as.matrix(testCoefs)
			testFeatures <- featureData[, row.names(testCoefs)]
			prediction <- testFeatures %*% testCoefs
			prediction <- prediction + method@linearModel$coefficients[1]
			return(prediction)
		}
)

LinearRegression <- function(){
  newObj <- new("LinearRegression")
}
@

Now evaluate a model using our customized class by passing an instance of this class as the method argument to crossValidatePredictiveModel()
<<>>=
myLinear_cv <- crossValidatePredictiveModel(t(featureData_scaled), 
                                            t(responseData_scaled), 
                                            method=LinearRegression(), 
                                            numFolds=10)
@


\section{Compare the performance of different models}
Now, compare the test errors from the elastic net model to the linear regression model to show that those of elastic net are far more accurate.

<<fig=TRUE>>=
boxplot(computeTestErrors(eNet_cv), 
        computeTestErrors(eNet_lowLambda_cv), 
        computeTestErrors(myLinear_cv), log="y")
@

\section{Next steps}
Implement your own customized predictive models and use the cross validation evaluation procedures to test if your method achieves more accurate predictive performance than currently used methods.

\section{Session Information}

<<sessionInfo, results=tex, print=TRUE>>=
toLatex(sessionInfo())
@

\end{document}
